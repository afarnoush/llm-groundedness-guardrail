# RAG-Based Chatbot Evaluation Framework

This project provides a robust evaluation framework to benchmark the performance of Retrieval-Augmented Generation (RAG) chatbots in a banking context. It enables comparisons across different LLM models, prompt strategies, and retrievers using real policy questions and document-grounded answers.

## ğŸ“Œ Objectives
- Ensure chatbot responses are accurate, grounded, and aligned with policy.
- Compare multiple prompts and models using standard NLP metrics.
- Establish guardrails for safe and compliant chatbot deployment.

## ğŸ§± Project Structure
```
â”œâ”€â”€ docsModel/                              # Markdown source documents for retriever
â”œâ”€â”€ Prompts/                                # Prompt templates (.txt)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rag_evaluator.py                     # Main evaluation function
â”‚   â”œâ”€â”€ retriever_builder.py                 # Ensemble retriever setup from markdown
â”‚   â””â”€â”€ utils.py                             # Shared utilities (BLEU, ROUGE, cosine, etc.)
â”œâ”€â”€ Outputs/                                 # (Optional) Saved evaluation results
â””â”€â”€ evaluation_notebook.ipynb                # Run experiments and visualize results
â””â”€â”€ CBA_Chatbot_Evaluation_30_Questions.csv  # Evaluation CSV files (questions + expected answers)
```

## âš™ï¸ Features
- **Evaluate LLMs** (e.g. Gemma, Phi3) on chatbot response quality
- **Compare prompts**: default, strict grounding, sales-focused, structured
- **Support for multiple metrics**:
  - Cosine Similarity (semantic match)
  - BLEU (precision)
  - ROUGE (recall)
  
## ğŸš€ How to Use
1. Place your evaluation questions in `your_questions.csv`
2. Save product/policy documents in markdown format under `docsModel/`
3. Add or edit prompt templates in `Prompts/`
4. Run `evaluation_notebook.ipynb` to:
   - Load prompts + models
   - Run `evaluate_rag()`
   - Generate metrics
   - Visualize with bar charts

## ğŸ“Š Outputs
Each evaluation run outputs a DataFrame (and optional `.csv`) with:
- Generated answer
- Cosine similarity to expected
- BLEU / ROUGE / Recall@K
- Prompt and model used
- Time taken per query

## ğŸ›¡ï¸ Guardrail Strategy
- Use Cosine Similarity to detect ungrounded responses
- Flag responses below a similarity threshold (e.g., < 0.6)
- Quantify and compare performance before deployment

## ğŸ“ˆ Example Visualizations
- Bar charts comparing BLEU, ROUGE, and Cosine across prompts and models

## âœ… Ideal For
- Risk and compliance testing of generative AI in banking
- Model and prompt A/B testing
- Safe scaling of RAG-based virtual assistants

## ğŸ‘¤ Author
Built by Azadeh Farnoush as part of a senior analytics innovation project.

